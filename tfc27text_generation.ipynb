{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfMtKSJbF00emDx4Zvel/p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hyorim-Kim/numpi/blob/main/tfc27text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tx2ieePMpaas",
        "outputId": "d03e51f4-8845-42d8-b17a-dbb1aaaeee6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 5, 51, 52, 53, 54, 55, 6, 7, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 3, 66, 67, 68, 69, 70, 71, 8, 72, 73, 9, 74, 75, 1, 10, 76, 77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99, 100, 16, 101, 102, 103, 104, 17, 18, 105, 106, 107, 108, 11, 109, 16, 110, 111, 13, 112, 113, 19, 114, 115, 116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 3, 142, 143, 144, 145, 2, 146, 147, 148, 149, 150, 4, 151, 152, 1, 153, 154, 22, 155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179, 180, 14, 181, 182, 15, 183, 184, 185, 186, 187, 19, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 22, 199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 25, 230, 231, 24, 232, 233, 1, 20, 234, 235, 12, 236, 237, 6, 7, 25, 238, 9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247, 2, 248, 4, 249, 2, 28, 250, 251, 252, 27, 17, 18, 253, 254, 255, 28, 26, 256, 257, 258, 259, 260, 261]\n",
            "{'희망퇴직': 1, '이상': 2, '희망퇴직을': 3, '만': 4, '수': 5, '눈치만': 6, '보는': 7, '중이다': 8, '지난달': 9, '신청을': 10, '올해': 11, '것은': 12, '퇴직금': 13, '때문이다': 14, '5대': 15, '희망': 16, '특별': 17, '퇴직금을': 18, '넘는': 19, '조건이': 20, '때': 21, '것이다': 22, '조건을': 23, '노조': 24, '상황”이라고': 25, '직원은': 26, '치': 27, '일반': 28, '은행권이': 29, '연말': 30, '실시를': 31, '앞두고': 32, '규모': 33, '등에': 34, '고심하고': 35, '있다': 36, '‘이자': 37, '장사’로': 38, '손쉽게': 39, '번': 40, '돈으로': 41, '성과급·퇴직금': 42, '잔치를': 43, '벌인다는': 44, '비판': 45, '여론이': 46, '높아진': 47, '상황을': 48, '의식하지': 49, '않을': 50, '없어서다': 51, '은행권': 52, '노조에선': 53, '“은행들': 54, '모두': 55, '상황”이라는': 56, '말이': 57, '나온다': 58, '4일': 59, '금융권에': 60, '따르면': 61, 'kb국민은행과': 62, '우리은행이': 63, '이달': 64, '말': 65, '실시할': 66, '전망이다': 67, '현재': 68, '임단협': 69, '협상을': 70, '진행': 71, '앞서': 72, 'nh농협은행은': 73, '21': 74, '23일까지': 75, '받았다': 76, '매년': 77, '정례화된': 78, '은행권의': 79, '희망퇴직이': 80, '유독': 81, '주목받는': 82, '은행들이': 83, '사회적': 84, '책임을': 85, '소홀히': 86, '하면서': 87, '성과급은': 88, '물론': 89, '잔치까지': 90, '벌인다고': 91, '정부와': 92, '여론으로부터': 93, '뭇매를': 94, '맞고': 95, '있기': 96, '실제로': 97, '지난해': 98, '은행에서만': 99, '2357명의': 100, '퇴직자가': 101, '1인당': 102, '평균': 103, '3억5547만원의': 104, '손에': 105, '쥐고': 106, '은행을': 107, '떠났다': 108, '상반기': 109, '퇴직자들': 110, '중에는': 111, '총액이': 112, '11억원을': 113, '경우도': 114, '있었다': 115, '은행들은': 116, '경영·인력': 117, '구조': 118, '효율화를': 119, '이유로': 120, '대대적으로': 121, '실시하고': 122, '은행원들은': 123, '좋을': 124, '그만두고': 125, '새로운': 126, '도전을': 127, '하려는': 128, '계산이': 129, '맞물리면서': 130, '희망퇴직자는': 131, '계속': 132, '늘어나는': 133, '추세다': 134, '신한은행은': 135, '연초에': 136, '이어': 137, '지난': 138, '8월': 139, '두': 140, '번째': 141, '실시하면서': 142, '대상을': 143, '‘근속연수': 144, '15년': 145, '1983년생': 146, '이전': 147, '출생자’까지': 148, '넓히기도': 149, '했다': 150, '나이로': 151, '30대가': 152, '대상자에': 153, '포함됐던': 154, '은행권도': 155, '이런': 156, '‘돈': 157, '잔치’': 158, '비판을': 159, '의식해': 160, '눈치를': 161, '보며': 162, '고심': 163, '실적이': 164, '좋은데': 165, '무턱대고': 166, '축소했다가': 167, '노조가': 168, '반발할': 169, '있고': 170, '그렇다고': 171, '최근': 172, '분위기를': 173, '고려했을': 174, '신경쓰지': 175, '않는': 176, '듯한': 177, '모습을': 178, '보이기도': 179, '어렵기': 180, 'kb국민·신한·하나·우리·nh농협은행': 181, '등': 182, '은행은': 183, '올': 184, '들어': 185, '3분기까지': 186, '30조원이': 187, '이자': 188, '이익': 189, '30조9366억원': 190, '을': 191, '거뒀다': 192, '작년': 193, '같은': 194, '기간보다': 195, '7': 196, '4': 197, '늘어난': 198, '한': 199, '시중은행': 200, '관계자는': 201, '“직원들이': 202, '고생해서': 203, '만들어': 204, '놓은': 205, '성과가': 206, '수치': 207, '실적': 208, '로': 209, '확인되는': 210, '만큼': 211, '직원에게': 212, '보상을': 213, '해주는': 214, '데': 215, '대해': 216, '이견은': 217, '없다”면서도': 218, '“하지만': 219, '외부': 220, '시선도': 221, '감안해야': 222, '하니': 223, '어떻게': 224, '풀어나갈': 225, '지는': 226, '열심히': 227, '논의를': 228, '하는': 229, '말했다': 230, '다른': 231, '관계자도': 232, '“아직': 233, '구체적으로': 234, '나온': 235, '없다”며': 236, '“모두': 237, '전했다': 238, '받은': 239, 'nh농협은행의': 240, '경우': 241, '다소': 242, '축소했다': 243, '지난해는': 244, '56세': 245, '28개월': 246, '10년': 247, '근속한': 248, '40세': 249, '직원에겐': 250, '20': 251, '39개월': 252, '지급하는': 253, '조건이었으나': 254, '올해는': 255, '차등': 256, '없이': 257, '최대': 258, '20개월치': 259, '위로금을': 260, '준다': 261}\n"
          ]
        }
      ],
      "source": [
        "# RNN을 이용한 텍스트 생성\n",
        "# 문맥을 반영해서 다음 단어를 예측하여 텍스트를 생성 - 다항분류\n",
        "# 수정하기 ***\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences, to_categorical\n",
        "import numpy as np\n",
        "from keras.layers import Embedding, Dense, LSTM, Flatten\n",
        "from keras.models import Sequential\n",
        "\n",
        "# text = \"\"\"거리에 하얀 눈이 쌓이고 있다\n",
        "# 그의 눈이 발게 빛난다\n",
        "# 눈이 내리고 있는 오후 거리는 눈이 부시다\"\"\"\n",
        "\n",
        "text = \"\"\"\n",
        "은행권이 연말 희망퇴직 실시를 앞두고 희망퇴직 규모 등에 고심하고 있다. ‘이자 장사’로 손쉽게 번 돈으로 성과급·퇴직금 잔치를 벌인다는 비판 여론이 높아진 상황을 의식하지 않을 수 없어서다. 은행권 노조에선 “은행들 모두 눈치만 보는 상황”이라는 말이 나온다.\n",
        "4일 금융권에 따르면 KB국민은행과 우리은행이 이달 말 희망퇴직을 실시할 전망이다. 현재 임단협 협상을 진행 중이다. 앞서 NH농협은행은 지난달 21~23일까지 희망퇴직 신청을 받았다.\n",
        "매년 정례화된 은행권의 희망퇴직이 올해 유독 주목받는 것은 은행들이 사회적 책임을 소홀히 하면서 성과급은 물론 퇴직금 잔치까지 벌인다고 정부와 여론으로부터 뭇매를 맞고 있기 때문이다. 실제로 지난해 5대 은행에서만 2357명의 희망 퇴직자가 1인당 평균 3억5547만원의 특별 퇴직금을 손에 쥐고 은행을 떠났다. 올해 상반기 희망 퇴직자들 중에는 퇴직금 총액이 11억원을 넘는 경우도 있었다.\n",
        "은행들은 경영·인력 구조 효율화를 이유로 대대적으로 희망퇴직을 실시하고, 은행원들은 희망퇴직 조건이 좋을 때 그만두고 새로운 도전을 하려는 계산이 맞물리면서 희망퇴직자는 계속 늘어나는 추세다. 신한은행은 연초에 이어 지난 8월 두 번째 희망퇴직을 실시하면서 대상을 ‘근속연수 15년 이상, 1983년생 이전 출생자’까지 넓히기도 했다. 만 나이로 30대가 희망퇴직 대상자에 포함됐던 것이다.\n",
        "은행권도 이런 ‘돈 잔치’ 비판을 의식해 눈치를 보며 고심 중이다. 실적이 좋은데 희망퇴직 조건을 무턱대고 축소했다가 노조가 반발할 수 있고, 그렇다고 최근 분위기를 고려했을 때 신경쓰지 않는 듯한 모습을 보이기도 어렵기 때문이다. KB국민·신한·하나·우리·NH농협은행 등 5대 은행은 올 들어 3분기까지 30조원이 넘는 이자 이익(30조9366억원)을 거뒀다. 작년 같은 기간보다 7.4% 늘어난 것이다.\n",
        "한 시중은행 노조 관계자는 “직원들이 고생해서 만들어 놓은 성과가 수치(실적)로 확인되는 만큼 직원에게 보상을 해주는 데 대해 이견은 없다”면서도 “하지만 외부 시선도 감안해야 하니 어떻게 풀어나갈 지는 열심히 논의를 하는 상황”이라고 말했다. 다른 노조 관계자도 “아직 희망퇴직 조건이 구체적으로 나온 것은 없다”며 “모두 눈치만 보는 상황”이라고 전했다.\n",
        "지난달 희망퇴직 신청을 받은 NH농협은행의 경우 희망퇴직 조건을 다소 축소했다. 지난해는 만 56세 이상 직원은 28개월 치, 10년 이상 근속한 만 40세 이상 일반 직원에겐 20~39개월 치 특별 퇴직금을 지급하는 조건이었으나, 올해는 일반 직원은 차등 없이 최대 20개월치 위로금을 준다.\n",
        "\"\"\"\n",
        "\n",
        "tok = Tokenizer()\n",
        "tok.fit_on_texts([text])\n",
        "encoded = tok.texts_to_sequences([text])[0]\n",
        "print(encoded)\n",
        "print(tok.word_index)\n",
        "\n",
        "vocab_size = len(tok.word_index) + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터\n",
        "sequences = list()\n",
        "for line in text.split('\\n'):   # 문장 토큰화\n",
        "  enco = tok.texts_to_sequences([line])[0]\n",
        "  # print(enco) # [2, 3, 1, 4, 5][6, 1, 7, 8][1, 9, 10, 11, 12, 1, 13]\n",
        "  for i in range(1, len(enco)):\n",
        "    sequ = enco[:i + 1]\n",
        "    # print(sequ)\n",
        "    sequences.append(sequ)\n",
        "\n",
        "print('학습에 참여할 샘플 수 : %d'%len(sequences))\n",
        "print(sequences)\n",
        "max_len = max(len(i) for i in sequences)  # 7\n",
        "\n",
        "psequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
        "print(psequences)\n",
        "\n",
        "x = psequences[:, :-1]\n",
        "y = psequences[:, -1]\n",
        "print(x)\n",
        "print(y)\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "print(y[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzF2ukCqswSZ",
        "outputId": "5fb0caa6-2cf2-4246-aa4c-f660f950218f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습에 참여할 샘플 수 : 293\n",
            "[[29, 30], [29, 30, 1], [29, 30, 1, 31], [29, 30, 1, 31, 32], [29, 30, 1, 31, 32, 1], [29, 30, 1, 31, 32, 1, 33], [29, 30, 1, 31, 32, 1, 33, 34], [29, 30, 1, 31, 32, 1, 33, 34, 35], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38, 39], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38, 39, 40], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38, 39, 40, 41], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 5], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 5, 51], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 5, 51, 52], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 5, 51, 52, 53], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 5, 51, 52, 53, 54], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 5, 51, 52, 53, 54, 55], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 5, 51, 52, 53, 54, 55, 6], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 5, 51, 52, 53, 54, 55, 6, 7], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 5, 51, 52, 53, 54, 55, 6, 7, 56], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 5, 51, 52, 53, 54, 55, 6, 7, 56, 57], [29, 30, 1, 31, 32, 1, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 5, 51, 52, 53, 54, 55, 6, 7, 56, 57, 58], [59, 60], [59, 60, 61], [59, 60, 61, 62], [59, 60, 61, 62, 63], [59, 60, 61, 62, 63, 64], [59, 60, 61, 62, 63, 64, 65], [59, 60, 61, 62, 63, 64, 65, 3], [59, 60, 61, 62, 63, 64, 65, 3, 66], [59, 60, 61, 62, 63, 64, 65, 3, 66, 67], [59, 60, 61, 62, 63, 64, 65, 3, 66, 67, 68], [59, 60, 61, 62, 63, 64, 65, 3, 66, 67, 68, 69], [59, 60, 61, 62, 63, 64, 65, 3, 66, 67, 68, 69, 70], [59, 60, 61, 62, 63, 64, 65, 3, 66, 67, 68, 69, 70, 71], [59, 60, 61, 62, 63, 64, 65, 3, 66, 67, 68, 69, 70, 71, 8], [59, 60, 61, 62, 63, 64, 65, 3, 66, 67, 68, 69, 70, 71, 8, 72], [59, 60, 61, 62, 63, 64, 65, 3, 66, 67, 68, 69, 70, 71, 8, 72, 73], [59, 60, 61, 62, 63, 64, 65, 3, 66, 67, 68, 69, 70, 71, 8, 72, 73, 9], [59, 60, 61, 62, 63, 64, 65, 3, 66, 67, 68, 69, 70, 71, 8, 72, 73, 9, 74], [59, 60, 61, 62, 63, 64, 65, 3, 66, 67, 68, 69, 70, 71, 8, 72, 73, 9, 74, 75], [59, 60, 61, 62, 63, 64, 65, 3, 66, 67, 68, 69, 70, 71, 8, 72, 73, 9, 74, 75, 1], [59, 60, 61, 62, 63, 64, 65, 3, 66, 67, 68, 69, 70, 71, 8, 72, 73, 9, 74, 75, 1, 10], [59, 60, 61, 62, 63, 64, 65, 3, 66, 67, 68, 69, 70, 71, 8, 72, 73, 9, 74, 75, 1, 10, 76], [77, 78], [77, 78, 79], [77, 78, 79, 80], [77, 78, 79, 80, 11], [77, 78, 79, 80, 11, 81], [77, 78, 79, 80, 11, 81, 82], [77, 78, 79, 80, 11, 81, 82, 12], [77, 78, 79, 80, 11, 81, 82, 12, 83], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99, 100], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99, 100, 16], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99, 100, 16, 101], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99, 100, 16, 101, 102], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99, 100, 16, 101, 102, 103], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99, 100, 16, 101, 102, 103, 104], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99, 100, 16, 101, 102, 103, 104, 17], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99, 100, 16, 101, 102, 103, 104, 17, 18], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99, 100, 16, 101, 102, 103, 104, 17, 18, 105], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99, 100, 16, 101, 102, 103, 104, 17, 18, 105, 106], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99, 100, 16, 101, 102, 103, 104, 17, 18, 105, 106, 107], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99, 100, 16, 101, 102, 103, 104, 17, 18, 105, 106, 107, 108], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99, 100, 16, 101, 102, 103, 104, 17, 18, 105, 106, 107, 108, 11], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99, 100, 16, 101, 102, 103, 104, 17, 18, 105, 106, 107, 108, 11, 109], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99, 100, 16, 101, 102, 103, 104, 17, 18, 105, 106, 107, 108, 11, 109, 16], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99, 100, 16, 101, 102, 103, 104, 17, 18, 105, 106, 107, 108, 11, 109, 16, 110], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99, 100, 16, 101, 102, 103, 104, 17, 18, 105, 106, 107, 108, 11, 109, 16, 110, 111], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99, 100, 16, 101, 102, 103, 104, 17, 18, 105, 106, 107, 108, 11, 109, 16, 110, 111, 13], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99, 100, 16, 101, 102, 103, 104, 17, 18, 105, 106, 107, 108, 11, 109, 16, 110, 111, 13, 112], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99, 100, 16, 101, 102, 103, 104, 17, 18, 105, 106, 107, 108, 11, 109, 16, 110, 111, 13, 112, 113], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99, 100, 16, 101, 102, 103, 104, 17, 18, 105, 106, 107, 108, 11, 109, 16, 110, 111, 13, 112, 113, 19], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99, 100, 16, 101, 102, 103, 104, 17, 18, 105, 106, 107, 108, 11, 109, 16, 110, 111, 13, 112, 113, 19, 114], [77, 78, 79, 80, 11, 81, 82, 12, 83, 84, 85, 86, 87, 88, 89, 13, 90, 91, 92, 93, 94, 95, 96, 14, 97, 98, 15, 99, 100, 16, 101, 102, 103, 104, 17, 18, 105, 106, 107, 108, 11, 109, 16, 110, 111, 13, 112, 113, 19, 114, 115], [116, 117], [116, 117, 118], [116, 117, 118, 119], [116, 117, 118, 119, 120], [116, 117, 118, 119, 120, 121], [116, 117, 118, 119, 120, 121, 3], [116, 117, 118, 119, 120, 121, 3, 122], [116, 117, 118, 119, 120, 121, 3, 122, 123], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 3], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 3, 142], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 3, 142, 143], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 3, 142, 143, 144], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 3, 142, 143, 144, 145], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 3, 142, 143, 144, 145, 2], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 3, 142, 143, 144, 145, 2, 146], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 3, 142, 143, 144, 145, 2, 146, 147], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 3, 142, 143, 144, 145, 2, 146, 147, 148], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 3, 142, 143, 144, 145, 2, 146, 147, 148, 149], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 3, 142, 143, 144, 145, 2, 146, 147, 148, 149, 150], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 3, 142, 143, 144, 145, 2, 146, 147, 148, 149, 150, 4], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 3, 142, 143, 144, 145, 2, 146, 147, 148, 149, 150, 4, 151], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 3, 142, 143, 144, 145, 2, 146, 147, 148, 149, 150, 4, 151, 152], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 3, 142, 143, 144, 145, 2, 146, 147, 148, 149, 150, 4, 151, 152, 1], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 3, 142, 143, 144, 145, 2, 146, 147, 148, 149, 150, 4, 151, 152, 1, 153], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 3, 142, 143, 144, 145, 2, 146, 147, 148, 149, 150, 4, 151, 152, 1, 153, 154], [116, 117, 118, 119, 120, 121, 3, 122, 123, 1, 20, 124, 21, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 3, 142, 143, 144, 145, 2, 146, 147, 148, 149, 150, 4, 151, 152, 1, 153, 154, 22], [155, 156], [155, 156, 157], [155, 156, 157, 158], [155, 156, 157, 158, 159], [155, 156, 157, 158, 159, 160], [155, 156, 157, 158, 159, 160, 161], [155, 156, 157, 158, 159, 160, 161, 162], [155, 156, 157, 158, 159, 160, 161, 162, 163], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179, 180], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179, 180, 14], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179, 180, 14, 181], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179, 180, 14, 181, 182], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179, 180, 14, 181, 182, 15], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179, 180, 14, 181, 182, 15, 183], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179, 180, 14, 181, 182, 15, 183, 184], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179, 180, 14, 181, 182, 15, 183, 184, 185], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179, 180, 14, 181, 182, 15, 183, 184, 185, 186], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179, 180, 14, 181, 182, 15, 183, 184, 185, 186, 187], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179, 180, 14, 181, 182, 15, 183, 184, 185, 186, 187, 19], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179, 180, 14, 181, 182, 15, 183, 184, 185, 186, 187, 19, 188], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179, 180, 14, 181, 182, 15, 183, 184, 185, 186, 187, 19, 188, 189], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179, 180, 14, 181, 182, 15, 183, 184, 185, 186, 187, 19, 188, 189, 190], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179, 180, 14, 181, 182, 15, 183, 184, 185, 186, 187, 19, 188, 189, 190, 191], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179, 180, 14, 181, 182, 15, 183, 184, 185, 186, 187, 19, 188, 189, 190, 191, 192], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179, 180, 14, 181, 182, 15, 183, 184, 185, 186, 187, 19, 188, 189, 190, 191, 192, 193], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179, 180, 14, 181, 182, 15, 183, 184, 185, 186, 187, 19, 188, 189, 190, 191, 192, 193, 194], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179, 180, 14, 181, 182, 15, 183, 184, 185, 186, 187, 19, 188, 189, 190, 191, 192, 193, 194, 195], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179, 180, 14, 181, 182, 15, 183, 184, 185, 186, 187, 19, 188, 189, 190, 191, 192, 193, 194, 195, 196], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179, 180, 14, 181, 182, 15, 183, 184, 185, 186, 187, 19, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179, 180, 14, 181, 182, 15, 183, 184, 185, 186, 187, 19, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198], [155, 156, 157, 158, 159, 160, 161, 162, 163, 8, 164, 165, 1, 23, 166, 167, 168, 169, 5, 170, 171, 172, 173, 174, 21, 175, 176, 177, 178, 179, 180, 14, 181, 182, 15, 183, 184, 185, 186, 187, 19, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 22], [199, 200], [199, 200, 24], [199, 200, 24, 201], [199, 200, 24, 201, 202], [199, 200, 24, 201, 202, 203], [199, 200, 24, 201, 202, 203, 204], [199, 200, 24, 201, 202, 203, 204, 205], [199, 200, 24, 201, 202, 203, 204, 205, 206], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 25], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 25, 230], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 25, 230, 231], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 25, 230, 231, 24], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 25, 230, 231, 24, 232], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 25, 230, 231, 24, 232, 233], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 25, 230, 231, 24, 232, 233, 1], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 25, 230, 231, 24, 232, 233, 1, 20], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 25, 230, 231, 24, 232, 233, 1, 20, 234], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 25, 230, 231, 24, 232, 233, 1, 20, 234, 235], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 25, 230, 231, 24, 232, 233, 1, 20, 234, 235, 12], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 25, 230, 231, 24, 232, 233, 1, 20, 234, 235, 12, 236], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 25, 230, 231, 24, 232, 233, 1, 20, 234, 235, 12, 236, 237], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 25, 230, 231, 24, 232, 233, 1, 20, 234, 235, 12, 236, 237, 6], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 25, 230, 231, 24, 232, 233, 1, 20, 234, 235, 12, 236, 237, 6, 7], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 25, 230, 231, 24, 232, 233, 1, 20, 234, 235, 12, 236, 237, 6, 7, 25], [199, 200, 24, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 25, 230, 231, 24, 232, 233, 1, 20, 234, 235, 12, 236, 237, 6, 7, 25, 238], [9, 1], [9, 1, 10], [9, 1, 10, 239], [9, 1, 10, 239, 240], [9, 1, 10, 239, 240, 241], [9, 1, 10, 239, 240, 241, 1], [9, 1, 10, 239, 240, 241, 1, 23], [9, 1, 10, 239, 240, 241, 1, 23, 242], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247, 2], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247, 2, 248], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247, 2, 248, 4], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247, 2, 248, 4, 249], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247, 2, 248, 4, 249, 2], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247, 2, 248, 4, 249, 2, 28], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247, 2, 248, 4, 249, 2, 28, 250], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247, 2, 248, 4, 249, 2, 28, 250, 251], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247, 2, 248, 4, 249, 2, 28, 250, 251, 252], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247, 2, 248, 4, 249, 2, 28, 250, 251, 252, 27], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247, 2, 248, 4, 249, 2, 28, 250, 251, 252, 27, 17], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247, 2, 248, 4, 249, 2, 28, 250, 251, 252, 27, 17, 18], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247, 2, 248, 4, 249, 2, 28, 250, 251, 252, 27, 17, 18, 253], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247, 2, 248, 4, 249, 2, 28, 250, 251, 252, 27, 17, 18, 253, 254], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247, 2, 248, 4, 249, 2, 28, 250, 251, 252, 27, 17, 18, 253, 254, 255], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247, 2, 248, 4, 249, 2, 28, 250, 251, 252, 27, 17, 18, 253, 254, 255, 28], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247, 2, 248, 4, 249, 2, 28, 250, 251, 252, 27, 17, 18, 253, 254, 255, 28, 26], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247, 2, 248, 4, 249, 2, 28, 250, 251, 252, 27, 17, 18, 253, 254, 255, 28, 26, 256], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247, 2, 248, 4, 249, 2, 28, 250, 251, 252, 27, 17, 18, 253, 254, 255, 28, 26, 256, 257], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247, 2, 248, 4, 249, 2, 28, 250, 251, 252, 27, 17, 18, 253, 254, 255, 28, 26, 256, 257, 258], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247, 2, 248, 4, 249, 2, 28, 250, 251, 252, 27, 17, 18, 253, 254, 255, 28, 26, 256, 257, 258, 259], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247, 2, 248, 4, 249, 2, 28, 250, 251, 252, 27, 17, 18, 253, 254, 255, 28, 26, 256, 257, 258, 259, 260], [9, 1, 10, 239, 240, 241, 1, 23, 242, 243, 244, 4, 245, 2, 26, 246, 27, 247, 2, 248, 4, 249, 2, 28, 250, 251, 252, 27, 17, 18, 253, 254, 255, 28, 26, 256, 257, 258, 259, 260, 261]]\n",
            "[[  0   0   0 ...   0  29  30]\n",
            " [  0   0   0 ...  29  30   1]\n",
            " [  0   0   0 ...  30   1  31]\n",
            " ...\n",
            " [  0   0   0 ... 257 258 259]\n",
            " [  0   0   0 ... 258 259 260]\n",
            " [  0   0   0 ... 259 260 261]]\n",
            "[[  0   0   0 ...   0   0  29]\n",
            " [  0   0   0 ...   0  29  30]\n",
            " [  0   0   0 ...  29  30   1]\n",
            " ...\n",
            " [  0   0   0 ... 256 257 258]\n",
            " [  0   0   0 ... 257 258 259]\n",
            " [  0   0   0 ... 258 259 260]]\n",
            "[ 30   1  31  32   1  33  34  35  36  37  38  39  40  41  42  43  44  45\n",
            "  46  47  48  49  50   5  51  52  53  54  55   6   7  56  57  58  60  61\n",
            "  62  63  64  65   3  66  67  68  69  70  71   8  72  73   9  74  75   1\n",
            "  10  76  78  79  80  11  81  82  12  83  84  85  86  87  88  89  13  90\n",
            "  91  92  93  94  95  96  14  97  98  15  99 100  16 101 102 103 104  17\n",
            "  18 105 106 107 108  11 109  16 110 111  13 112 113  19 114 115 117 118\n",
            " 119 120 121   3 122 123   1  20 124  21 125 126 127 128 129 130 131 132\n",
            " 133 134 135 136 137 138 139 140 141   3 142 143 144 145   2 146 147 148\n",
            " 149 150   4 151 152   1 153 154  22 156 157 158 159 160 161 162 163   8\n",
            " 164 165   1  23 166 167 168 169   5 170 171 172 173 174  21 175 176 177\n",
            " 178 179 180  14 181 182  15 183 184 185 186 187  19 188 189 190 191 192\n",
            " 193 194 195 196 197 198  22 200  24 201 202 203 204 205 206 207 208 209\n",
            " 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227\n",
            " 228 229  25 230 231  24 232 233   1  20 234 235  12 236 237   6   7  25\n",
            " 238   1  10 239 240 241   1  23 242 243 244   4 245   2  26 246  27 247\n",
            "   2 248   4 249   2  28 250 251 252  27  17  18 253 254 255  28  26 256\n",
            " 257 258 259 260 261]\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model= Sequential()\n",
        "model.add(Embedding(vocab_size, 32, input_length=max_len-1))\n",
        "model.add(LSTM(32,activation='tanh'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32,activation='tanh'))\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dense(vocab_size,activation='softmax'))\n",
        "print(model.summary())\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x,y, epochs=200, verbose=2)\n",
        "print('model.evaluate :', model.evaluate(x,y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPKtDeR7ytd5",
        "outputId": "c92b1777-2a4a-4d60-a055-91f0a6cc614e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 52, 32)            8384      \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 32)                8320      \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 262)               8646      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 27462 (107.27 KB)\n",
            "Trainable params: 27462 (107.27 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "10/10 - 4s - loss: 5.5702 - accuracy: 0.0102 - 4s/epoch - 366ms/step\n",
            "Epoch 2/200\n",
            "10/10 - 0s - loss: 5.5663 - accuracy: 0.0341 - 339ms/epoch - 34ms/step\n",
            "Epoch 3/200\n",
            "10/10 - 0s - loss: 5.5629 - accuracy: 0.0307 - 345ms/epoch - 34ms/step\n",
            "Epoch 4/200\n",
            "10/10 - 0s - loss: 5.5569 - accuracy: 0.0307 - 325ms/epoch - 32ms/step\n",
            "Epoch 5/200\n",
            "10/10 - 0s - loss: 5.5403 - accuracy: 0.0307 - 320ms/epoch - 32ms/step\n",
            "Epoch 6/200\n",
            "10/10 - 0s - loss: 5.5002 - accuracy: 0.0307 - 314ms/epoch - 31ms/step\n",
            "Epoch 7/200\n",
            "10/10 - 0s - loss: 5.4507 - accuracy: 0.0307 - 318ms/epoch - 32ms/step\n",
            "Epoch 8/200\n",
            "10/10 - 0s - loss: 5.3760 - accuracy: 0.0307 - 294ms/epoch - 29ms/step\n",
            "Epoch 9/200\n",
            "10/10 - 0s - loss: 5.2916 - accuracy: 0.0307 - 223ms/epoch - 22ms/step\n",
            "Epoch 10/200\n",
            "10/10 - 0s - loss: 5.2084 - accuracy: 0.0375 - 216ms/epoch - 22ms/step\n",
            "Epoch 11/200\n",
            "10/10 - 0s - loss: 5.0955 - accuracy: 0.0375 - 219ms/epoch - 22ms/step\n",
            "Epoch 12/200\n",
            "10/10 - 0s - loss: 4.9796 - accuracy: 0.0410 - 220ms/epoch - 22ms/step\n",
            "Epoch 13/200\n",
            "10/10 - 0s - loss: 4.8653 - accuracy: 0.0341 - 227ms/epoch - 23ms/step\n",
            "Epoch 14/200\n",
            "10/10 - 0s - loss: 4.7781 - accuracy: 0.0341 - 221ms/epoch - 22ms/step\n",
            "Epoch 15/200\n",
            "10/10 - 0s - loss: 4.6767 - accuracy: 0.0444 - 216ms/epoch - 22ms/step\n",
            "Epoch 16/200\n",
            "10/10 - 0s - loss: 4.5650 - accuracy: 0.0410 - 226ms/epoch - 23ms/step\n",
            "Epoch 17/200\n",
            "10/10 - 0s - loss: 4.4493 - accuracy: 0.0478 - 226ms/epoch - 23ms/step\n",
            "Epoch 18/200\n",
            "10/10 - 0s - loss: 4.3441 - accuracy: 0.0546 - 225ms/epoch - 23ms/step\n",
            "Epoch 19/200\n",
            "10/10 - 0s - loss: 4.2507 - accuracy: 0.0683 - 217ms/epoch - 22ms/step\n",
            "Epoch 20/200\n",
            "10/10 - 0s - loss: 4.1512 - accuracy: 0.0648 - 215ms/epoch - 22ms/step\n",
            "Epoch 21/200\n",
            "10/10 - 0s - loss: 4.0610 - accuracy: 0.0751 - 222ms/epoch - 22ms/step\n",
            "Epoch 22/200\n",
            "10/10 - 0s - loss: 3.9692 - accuracy: 0.0785 - 222ms/epoch - 22ms/step\n",
            "Epoch 23/200\n",
            "10/10 - 0s - loss: 3.8882 - accuracy: 0.0751 - 223ms/epoch - 22ms/step\n",
            "Epoch 24/200\n",
            "10/10 - 0s - loss: 3.8047 - accuracy: 0.0853 - 228ms/epoch - 23ms/step\n",
            "Epoch 25/200\n",
            "10/10 - 0s - loss: 3.7241 - accuracy: 0.0922 - 242ms/epoch - 24ms/step\n",
            "Epoch 26/200\n",
            "10/10 - 0s - loss: 3.6542 - accuracy: 0.0990 - 221ms/epoch - 22ms/step\n",
            "Epoch 27/200\n",
            "10/10 - 0s - loss: 3.5867 - accuracy: 0.1024 - 224ms/epoch - 22ms/step\n",
            "Epoch 28/200\n",
            "10/10 - 0s - loss: 3.5224 - accuracy: 0.1160 - 209ms/epoch - 21ms/step\n",
            "Epoch 29/200\n",
            "10/10 - 0s - loss: 3.4553 - accuracy: 0.1229 - 216ms/epoch - 22ms/step\n",
            "Epoch 30/200\n",
            "10/10 - 0s - loss: 3.3979 - accuracy: 0.1092 - 225ms/epoch - 23ms/step\n",
            "Epoch 31/200\n",
            "10/10 - 0s - loss: 3.3418 - accuracy: 0.1092 - 232ms/epoch - 23ms/step\n",
            "Epoch 32/200\n",
            "10/10 - 0s - loss: 3.2780 - accuracy: 0.1365 - 217ms/epoch - 22ms/step\n",
            "Epoch 33/200\n",
            "10/10 - 0s - loss: 3.2261 - accuracy: 0.1433 - 219ms/epoch - 22ms/step\n",
            "Epoch 34/200\n",
            "10/10 - 0s - loss: 3.1831 - accuracy: 0.1468 - 215ms/epoch - 21ms/step\n",
            "Epoch 35/200\n",
            "10/10 - 0s - loss: 3.1283 - accuracy: 0.1468 - 217ms/epoch - 22ms/step\n",
            "Epoch 36/200\n",
            "10/10 - 0s - loss: 3.0814 - accuracy: 0.1570 - 232ms/epoch - 23ms/step\n",
            "Epoch 37/200\n",
            "10/10 - 0s - loss: 3.0259 - accuracy: 0.1570 - 218ms/epoch - 22ms/step\n",
            "Epoch 38/200\n",
            "10/10 - 0s - loss: 2.9861 - accuracy: 0.1706 - 216ms/epoch - 22ms/step\n",
            "Epoch 39/200\n",
            "10/10 - 0s - loss: 2.9411 - accuracy: 0.1843 - 365ms/epoch - 36ms/step\n",
            "Epoch 40/200\n",
            "10/10 - 0s - loss: 2.8923 - accuracy: 0.1980 - 227ms/epoch - 23ms/step\n",
            "Epoch 41/200\n",
            "10/10 - 0s - loss: 2.8750 - accuracy: 0.2082 - 229ms/epoch - 23ms/step\n",
            "Epoch 42/200\n",
            "10/10 - 0s - loss: 2.8205 - accuracy: 0.2116 - 393ms/epoch - 39ms/step\n",
            "Epoch 43/200\n",
            "10/10 - 0s - loss: 2.7753 - accuracy: 0.2253 - 238ms/epoch - 24ms/step\n",
            "Epoch 44/200\n",
            "10/10 - 0s - loss: 2.7284 - accuracy: 0.2389 - 223ms/epoch - 22ms/step\n",
            "Epoch 45/200\n",
            "10/10 - 0s - loss: 2.6861 - accuracy: 0.2457 - 380ms/epoch - 38ms/step\n",
            "Epoch 46/200\n",
            "10/10 - 0s - loss: 2.6565 - accuracy: 0.2389 - 223ms/epoch - 22ms/step\n",
            "Epoch 47/200\n",
            "10/10 - 0s - loss: 2.6169 - accuracy: 0.2696 - 221ms/epoch - 22ms/step\n",
            "Epoch 48/200\n",
            "10/10 - 0s - loss: 2.5863 - accuracy: 0.2662 - 218ms/epoch - 22ms/step\n",
            "Epoch 49/200\n",
            "10/10 - 0s - loss: 2.5595 - accuracy: 0.2696 - 215ms/epoch - 21ms/step\n",
            "Epoch 50/200\n",
            "10/10 - 0s - loss: 2.5285 - accuracy: 0.2765 - 264ms/epoch - 26ms/step\n",
            "Epoch 51/200\n",
            "10/10 - 0s - loss: 2.4974 - accuracy: 0.2799 - 340ms/epoch - 34ms/step\n",
            "Epoch 52/200\n",
            "10/10 - 0s - loss: 2.4671 - accuracy: 0.3345 - 331ms/epoch - 33ms/step\n",
            "Epoch 53/200\n",
            "10/10 - 0s - loss: 2.4283 - accuracy: 0.2935 - 333ms/epoch - 33ms/step\n",
            "Epoch 54/200\n",
            "10/10 - 0s - loss: 2.3913 - accuracy: 0.3311 - 330ms/epoch - 33ms/step\n",
            "Epoch 55/200\n",
            "10/10 - 0s - loss: 2.3585 - accuracy: 0.3106 - 314ms/epoch - 31ms/step\n",
            "Epoch 56/200\n",
            "10/10 - 0s - loss: 2.3191 - accuracy: 0.3447 - 319ms/epoch - 32ms/step\n",
            "Epoch 57/200\n",
            "10/10 - 0s - loss: 2.2969 - accuracy: 0.3549 - 332ms/epoch - 33ms/step\n",
            "Epoch 58/200\n",
            "10/10 - 0s - loss: 2.2627 - accuracy: 0.3891 - 325ms/epoch - 32ms/step\n",
            "Epoch 59/200\n",
            "10/10 - 0s - loss: 2.2296 - accuracy: 0.3993 - 315ms/epoch - 31ms/step\n",
            "Epoch 60/200\n",
            "10/10 - 0s - loss: 2.2045 - accuracy: 0.3891 - 310ms/epoch - 31ms/step\n",
            "Epoch 61/200\n",
            "10/10 - 0s - loss: 2.1715 - accuracy: 0.4334 - 331ms/epoch - 33ms/step\n",
            "Epoch 62/200\n",
            "10/10 - 0s - loss: 2.1547 - accuracy: 0.4198 - 318ms/epoch - 32ms/step\n",
            "Epoch 63/200\n",
            "10/10 - 0s - loss: 2.1249 - accuracy: 0.4573 - 225ms/epoch - 22ms/step\n",
            "Epoch 64/200\n",
            "10/10 - 0s - loss: 2.0964 - accuracy: 0.4608 - 220ms/epoch - 22ms/step\n",
            "Epoch 65/200\n",
            "10/10 - 0s - loss: 2.0627 - accuracy: 0.4676 - 222ms/epoch - 22ms/step\n",
            "Epoch 66/200\n",
            "10/10 - 0s - loss: 2.0484 - accuracy: 0.4812 - 224ms/epoch - 22ms/step\n",
            "Epoch 67/200\n",
            "10/10 - 0s - loss: 2.0226 - accuracy: 0.4573 - 218ms/epoch - 22ms/step\n",
            "Epoch 68/200\n",
            "10/10 - 0s - loss: 1.9960 - accuracy: 0.4539 - 208ms/epoch - 21ms/step\n",
            "Epoch 69/200\n",
            "10/10 - 0s - loss: 1.9761 - accuracy: 0.4915 - 216ms/epoch - 22ms/step\n",
            "Epoch 70/200\n",
            "10/10 - 0s - loss: 1.9625 - accuracy: 0.4778 - 218ms/epoch - 22ms/step\n",
            "Epoch 71/200\n",
            "10/10 - 0s - loss: 1.9334 - accuracy: 0.4744 - 220ms/epoch - 22ms/step\n",
            "Epoch 72/200\n",
            "10/10 - 0s - loss: 1.9063 - accuracy: 0.5188 - 209ms/epoch - 21ms/step\n",
            "Epoch 73/200\n",
            "10/10 - 0s - loss: 1.8809 - accuracy: 0.5188 - 219ms/epoch - 22ms/step\n",
            "Epoch 74/200\n",
            "10/10 - 0s - loss: 1.8468 - accuracy: 0.5222 - 226ms/epoch - 23ms/step\n",
            "Epoch 75/200\n",
            "10/10 - 0s - loss: 1.8393 - accuracy: 0.5290 - 233ms/epoch - 23ms/step\n",
            "Epoch 76/200\n",
            "10/10 - 0s - loss: 1.8209 - accuracy: 0.5119 - 225ms/epoch - 23ms/step\n",
            "Epoch 77/200\n",
            "10/10 - 0s - loss: 1.7845 - accuracy: 0.5700 - 216ms/epoch - 22ms/step\n",
            "Epoch 78/200\n",
            "10/10 - 0s - loss: 1.7604 - accuracy: 0.5734 - 231ms/epoch - 23ms/step\n",
            "Epoch 79/200\n",
            "10/10 - 0s - loss: 1.7305 - accuracy: 0.5768 - 228ms/epoch - 23ms/step\n",
            "Epoch 80/200\n",
            "10/10 - 0s - loss: 1.7128 - accuracy: 0.5802 - 215ms/epoch - 22ms/step\n",
            "Epoch 81/200\n",
            "10/10 - 0s - loss: 1.6942 - accuracy: 0.5563 - 217ms/epoch - 22ms/step\n",
            "Epoch 82/200\n",
            "10/10 - 0s - loss: 1.6774 - accuracy: 0.5631 - 218ms/epoch - 22ms/step\n",
            "Epoch 83/200\n",
            "10/10 - 0s - loss: 1.6550 - accuracy: 0.6075 - 226ms/epoch - 23ms/step\n",
            "Epoch 84/200\n",
            "10/10 - 0s - loss: 1.6266 - accuracy: 0.6280 - 219ms/epoch - 22ms/step\n",
            "Epoch 85/200\n",
            "10/10 - 0s - loss: 1.6139 - accuracy: 0.6382 - 217ms/epoch - 22ms/step\n",
            "Epoch 86/200\n",
            "10/10 - 0s - loss: 1.5930 - accuracy: 0.6382 - 221ms/epoch - 22ms/step\n",
            "Epoch 87/200\n",
            "10/10 - 0s - loss: 1.5697 - accuracy: 0.6416 - 230ms/epoch - 23ms/step\n",
            "Epoch 88/200\n",
            "10/10 - 0s - loss: 1.5508 - accuracy: 0.6621 - 223ms/epoch - 22ms/step\n",
            "Epoch 89/200\n",
            "10/10 - 0s - loss: 1.5257 - accuracy: 0.6962 - 212ms/epoch - 21ms/step\n",
            "Epoch 90/200\n",
            "10/10 - 0s - loss: 1.5078 - accuracy: 0.6928 - 219ms/epoch - 22ms/step\n",
            "Epoch 91/200\n",
            "10/10 - 0s - loss: 1.4901 - accuracy: 0.6758 - 219ms/epoch - 22ms/step\n",
            "Epoch 92/200\n",
            "10/10 - 0s - loss: 1.4653 - accuracy: 0.6553 - 230ms/epoch - 23ms/step\n",
            "Epoch 93/200\n",
            "10/10 - 0s - loss: 1.4493 - accuracy: 0.6894 - 228ms/epoch - 23ms/step\n",
            "Epoch 94/200\n",
            "10/10 - 0s - loss: 1.4404 - accuracy: 0.6894 - 209ms/epoch - 21ms/step\n",
            "Epoch 95/200\n",
            "10/10 - 0s - loss: 1.4457 - accuracy: 0.6416 - 220ms/epoch - 22ms/step\n",
            "Epoch 96/200\n",
            "10/10 - 0s - loss: 1.4296 - accuracy: 0.6860 - 226ms/epoch - 23ms/step\n",
            "Epoch 97/200\n",
            "10/10 - 0s - loss: 1.3955 - accuracy: 0.7338 - 221ms/epoch - 22ms/step\n",
            "Epoch 98/200\n",
            "10/10 - 0s - loss: 1.4000 - accuracy: 0.6758 - 217ms/epoch - 22ms/step\n",
            "Epoch 99/200\n",
            "10/10 - 0s - loss: 1.3523 - accuracy: 0.7031 - 223ms/epoch - 22ms/step\n",
            "Epoch 100/200\n",
            "10/10 - 0s - loss: 1.3216 - accuracy: 0.7304 - 215ms/epoch - 22ms/step\n",
            "Epoch 101/200\n",
            "10/10 - 0s - loss: 1.2925 - accuracy: 0.7645 - 245ms/epoch - 24ms/step\n",
            "Epoch 102/200\n",
            "10/10 - 0s - loss: 1.2687 - accuracy: 0.7713 - 222ms/epoch - 22ms/step\n",
            "Epoch 103/200\n",
            "10/10 - 0s - loss: 1.2590 - accuracy: 0.7645 - 220ms/epoch - 22ms/step\n",
            "Epoch 104/200\n",
            "10/10 - 0s - loss: 1.2465 - accuracy: 0.7577 - 214ms/epoch - 21ms/step\n",
            "Epoch 105/200\n",
            "10/10 - 0s - loss: 1.2224 - accuracy: 0.7884 - 231ms/epoch - 23ms/step\n",
            "Epoch 106/200\n",
            "10/10 - 0s - loss: 1.2046 - accuracy: 0.7713 - 234ms/epoch - 23ms/step\n",
            "Epoch 107/200\n",
            "10/10 - 0s - loss: 1.2028 - accuracy: 0.7782 - 307ms/epoch - 31ms/step\n",
            "Epoch 108/200\n",
            "10/10 - 0s - loss: 1.1771 - accuracy: 0.8089 - 322ms/epoch - 32ms/step\n",
            "Epoch 109/200\n",
            "10/10 - 0s - loss: 1.1502 - accuracy: 0.8225 - 336ms/epoch - 34ms/step\n",
            "Epoch 110/200\n",
            "10/10 - 0s - loss: 1.1393 - accuracy: 0.8191 - 344ms/epoch - 34ms/step\n",
            "Epoch 111/200\n",
            "10/10 - 0s - loss: 1.1213 - accuracy: 0.8123 - 322ms/epoch - 32ms/step\n",
            "Epoch 112/200\n",
            "10/10 - 0s - loss: 1.0976 - accuracy: 0.8294 - 318ms/epoch - 32ms/step\n",
            "Epoch 113/200\n",
            "10/10 - 0s - loss: 1.0798 - accuracy: 0.8498 - 324ms/epoch - 32ms/step\n",
            "Epoch 114/200\n",
            "10/10 - 0s - loss: 1.0642 - accuracy: 0.8430 - 325ms/epoch - 33ms/step\n",
            "Epoch 115/200\n",
            "10/10 - 0s - loss: 1.0589 - accuracy: 0.8328 - 324ms/epoch - 32ms/step\n",
            "Epoch 116/200\n",
            "10/10 - 0s - loss: 1.0400 - accuracy: 0.8362 - 314ms/epoch - 31ms/step\n",
            "Epoch 117/200\n",
            "10/10 - 0s - loss: 1.0142 - accuracy: 0.8498 - 314ms/epoch - 31ms/step\n",
            "Epoch 118/200\n",
            "10/10 - 0s - loss: 0.9978 - accuracy: 0.8601 - 333ms/epoch - 33ms/step\n",
            "Epoch 119/200\n",
            "10/10 - 0s - loss: 0.9945 - accuracy: 0.8430 - 272ms/epoch - 27ms/step\n",
            "Epoch 120/200\n",
            "10/10 - 0s - loss: 0.9827 - accuracy: 0.8498 - 213ms/epoch - 21ms/step\n",
            "Epoch 121/200\n",
            "10/10 - 0s - loss: 0.9603 - accuracy: 0.8737 - 218ms/epoch - 22ms/step\n",
            "Epoch 122/200\n",
            "10/10 - 0s - loss: 0.9494 - accuracy: 0.8635 - 218ms/epoch - 22ms/step\n",
            "Epoch 123/200\n",
            "10/10 - 0s - loss: 0.9319 - accuracy: 0.8703 - 207ms/epoch - 21ms/step\n",
            "Epoch 124/200\n",
            "10/10 - 0s - loss: 0.9293 - accuracy: 0.8703 - 220ms/epoch - 22ms/step\n",
            "Epoch 125/200\n",
            "10/10 - 0s - loss: 0.9090 - accuracy: 0.8840 - 229ms/epoch - 23ms/step\n",
            "Epoch 126/200\n",
            "10/10 - 0s - loss: 0.8901 - accuracy: 0.9078 - 216ms/epoch - 22ms/step\n",
            "Epoch 127/200\n",
            "10/10 - 0s - loss: 0.8774 - accuracy: 0.8976 - 226ms/epoch - 23ms/step\n",
            "Epoch 128/200\n",
            "10/10 - 0s - loss: 0.8665 - accuracy: 0.9113 - 214ms/epoch - 21ms/step\n",
            "Epoch 129/200\n",
            "10/10 - 0s - loss: 0.8409 - accuracy: 0.9078 - 210ms/epoch - 21ms/step\n",
            "Epoch 130/200\n",
            "10/10 - 0s - loss: 0.8332 - accuracy: 0.8976 - 219ms/epoch - 22ms/step\n",
            "Epoch 131/200\n",
            "10/10 - 0s - loss: 0.8174 - accuracy: 0.8840 - 224ms/epoch - 22ms/step\n",
            "Epoch 132/200\n",
            "10/10 - 0s - loss: 0.8098 - accuracy: 0.8942 - 228ms/epoch - 23ms/step\n",
            "Epoch 133/200\n",
            "10/10 - 0s - loss: 0.7973 - accuracy: 0.9113 - 215ms/epoch - 21ms/step\n",
            "Epoch 134/200\n",
            "10/10 - 0s - loss: 0.7899 - accuracy: 0.9010 - 217ms/epoch - 22ms/step\n",
            "Epoch 135/200\n",
            "10/10 - 0s - loss: 0.7635 - accuracy: 0.9215 - 212ms/epoch - 21ms/step\n",
            "Epoch 136/200\n",
            "10/10 - 0s - loss: 0.7561 - accuracy: 0.9113 - 241ms/epoch - 24ms/step\n",
            "Epoch 137/200\n",
            "10/10 - 0s - loss: 0.7423 - accuracy: 0.9078 - 214ms/epoch - 21ms/step\n",
            "Epoch 138/200\n",
            "10/10 - 0s - loss: 0.7300 - accuracy: 0.9181 - 221ms/epoch - 22ms/step\n",
            "Epoch 139/200\n",
            "10/10 - 0s - loss: 0.7216 - accuracy: 0.9249 - 218ms/epoch - 22ms/step\n",
            "Epoch 140/200\n",
            "10/10 - 0s - loss: 0.7034 - accuracy: 0.9420 - 219ms/epoch - 22ms/step\n",
            "Epoch 141/200\n",
            "10/10 - 0s - loss: 0.6960 - accuracy: 0.9386 - 220ms/epoch - 22ms/step\n",
            "Epoch 142/200\n",
            "10/10 - 0s - loss: 0.6817 - accuracy: 0.9283 - 208ms/epoch - 21ms/step\n",
            "Epoch 143/200\n",
            "10/10 - 0s - loss: 0.6711 - accuracy: 0.9386 - 207ms/epoch - 21ms/step\n",
            "Epoch 144/200\n",
            "10/10 - 0s - loss: 0.6487 - accuracy: 0.9488 - 208ms/epoch - 21ms/step\n",
            "Epoch 145/200\n",
            "10/10 - 0s - loss: 0.6392 - accuracy: 0.9522 - 233ms/epoch - 23ms/step\n",
            "Epoch 146/200\n",
            "10/10 - 0s - loss: 0.6250 - accuracy: 0.9488 - 216ms/epoch - 22ms/step\n",
            "Epoch 147/200\n",
            "10/10 - 0s - loss: 0.6106 - accuracy: 0.9590 - 211ms/epoch - 21ms/step\n",
            "Epoch 148/200\n",
            "10/10 - 0s - loss: 0.6070 - accuracy: 0.9625 - 214ms/epoch - 21ms/step\n",
            "Epoch 149/200\n",
            "10/10 - 0s - loss: 0.6068 - accuracy: 0.9522 - 218ms/epoch - 22ms/step\n",
            "Epoch 150/200\n",
            "10/10 - 0s - loss: 0.5901 - accuracy: 0.9420 - 215ms/epoch - 22ms/step\n",
            "Epoch 151/200\n",
            "10/10 - 0s - loss: 0.5787 - accuracy: 0.9386 - 216ms/epoch - 22ms/step\n",
            "Epoch 152/200\n",
            "10/10 - 0s - loss: 0.5701 - accuracy: 0.9522 - 210ms/epoch - 21ms/step\n",
            "Epoch 153/200\n",
            "10/10 - 0s - loss: 0.5551 - accuracy: 0.9625 - 223ms/epoch - 22ms/step\n",
            "Epoch 154/200\n",
            "10/10 - 0s - loss: 0.5483 - accuracy: 0.9659 - 223ms/epoch - 22ms/step\n",
            "Epoch 155/200\n",
            "10/10 - 0s - loss: 0.5388 - accuracy: 0.9727 - 227ms/epoch - 23ms/step\n",
            "Epoch 156/200\n",
            "10/10 - 0s - loss: 0.5265 - accuracy: 0.9795 - 210ms/epoch - 21ms/step\n",
            "Epoch 157/200\n",
            "10/10 - 0s - loss: 0.5150 - accuracy: 0.9693 - 227ms/epoch - 23ms/step\n",
            "Epoch 158/200\n",
            "10/10 - 0s - loss: 0.5072 - accuracy: 0.9693 - 223ms/epoch - 22ms/step\n",
            "Epoch 159/200\n",
            "10/10 - 0s - loss: 0.4955 - accuracy: 0.9829 - 221ms/epoch - 22ms/step\n",
            "Epoch 160/200\n",
            "10/10 - 0s - loss: 0.4875 - accuracy: 0.9795 - 208ms/epoch - 21ms/step\n",
            "Epoch 161/200\n",
            "10/10 - 0s - loss: 0.4826 - accuracy: 0.9863 - 215ms/epoch - 22ms/step\n",
            "Epoch 162/200\n",
            "10/10 - 0s - loss: 0.4735 - accuracy: 0.9727 - 224ms/epoch - 22ms/step\n",
            "Epoch 163/200\n",
            "10/10 - 0s - loss: 0.4620 - accuracy: 0.9761 - 214ms/epoch - 21ms/step\n",
            "Epoch 164/200\n",
            "10/10 - 0s - loss: 0.4523 - accuracy: 0.9795 - 298ms/epoch - 30ms/step\n",
            "Epoch 165/200\n",
            "10/10 - 0s - loss: 0.4468 - accuracy: 0.9863 - 318ms/epoch - 32ms/step\n",
            "Epoch 166/200\n",
            "10/10 - 0s - loss: 0.4441 - accuracy: 0.9795 - 333ms/epoch - 33ms/step\n",
            "Epoch 167/200\n",
            "10/10 - 0s - loss: 0.4415 - accuracy: 0.9761 - 335ms/epoch - 33ms/step\n",
            "Epoch 168/200\n",
            "10/10 - 0s - loss: 0.4367 - accuracy: 0.9795 - 346ms/epoch - 35ms/step\n",
            "Epoch 169/200\n",
            "10/10 - 0s - loss: 0.4236 - accuracy: 0.9795 - 319ms/epoch - 32ms/step\n",
            "Epoch 170/200\n",
            "10/10 - 0s - loss: 0.4159 - accuracy: 0.9727 - 335ms/epoch - 34ms/step\n",
            "Epoch 171/200\n",
            "10/10 - 0s - loss: 0.4055 - accuracy: 0.9761 - 340ms/epoch - 34ms/step\n",
            "Epoch 172/200\n",
            "10/10 - 0s - loss: 0.3946 - accuracy: 0.9761 - 316ms/epoch - 32ms/step\n",
            "Epoch 173/200\n",
            "10/10 - 0s - loss: 0.3866 - accuracy: 0.9898 - 313ms/epoch - 31ms/step\n",
            "Epoch 174/200\n",
            "10/10 - 0s - loss: 0.3867 - accuracy: 0.9829 - 322ms/epoch - 32ms/step\n",
            "Epoch 175/200\n",
            "10/10 - 0s - loss: 0.3772 - accuracy: 0.9829 - 316ms/epoch - 32ms/step\n",
            "Epoch 176/200\n",
            "10/10 - 0s - loss: 0.3733 - accuracy: 0.9829 - 266ms/epoch - 27ms/step\n",
            "Epoch 177/200\n",
            "10/10 - 0s - loss: 0.3647 - accuracy: 0.9863 - 224ms/epoch - 22ms/step\n",
            "Epoch 178/200\n",
            "10/10 - 0s - loss: 0.3575 - accuracy: 0.9898 - 209ms/epoch - 21ms/step\n",
            "Epoch 179/200\n",
            "10/10 - 0s - loss: 0.3540 - accuracy: 0.9898 - 220ms/epoch - 22ms/step\n",
            "Epoch 180/200\n",
            "10/10 - 0s - loss: 0.3502 - accuracy: 0.9932 - 212ms/epoch - 21ms/step\n",
            "Epoch 181/200\n",
            "10/10 - 0s - loss: 0.3388 - accuracy: 0.9898 - 224ms/epoch - 22ms/step\n",
            "Epoch 182/200\n",
            "10/10 - 0s - loss: 0.3317 - accuracy: 0.9932 - 218ms/epoch - 22ms/step\n",
            "Epoch 183/200\n",
            "10/10 - 0s - loss: 0.3271 - accuracy: 0.9898 - 225ms/epoch - 22ms/step\n",
            "Epoch 184/200\n",
            "10/10 - 0s - loss: 0.3187 - accuracy: 0.9863 - 216ms/epoch - 22ms/step\n",
            "Epoch 185/200\n",
            "10/10 - 0s - loss: 0.3203 - accuracy: 0.9863 - 213ms/epoch - 21ms/step\n",
            "Epoch 186/200\n",
            "10/10 - 0s - loss: 0.3111 - accuracy: 0.9966 - 226ms/epoch - 23ms/step\n",
            "Epoch 187/200\n",
            "10/10 - 0s - loss: 0.3029 - accuracy: 1.0000 - 216ms/epoch - 22ms/step\n",
            "Epoch 188/200\n",
            "10/10 - 0s - loss: 0.2973 - accuracy: 0.9966 - 214ms/epoch - 21ms/step\n",
            "Epoch 189/200\n",
            "10/10 - 0s - loss: 0.2909 - accuracy: 0.9932 - 218ms/epoch - 22ms/step\n",
            "Epoch 190/200\n",
            "10/10 - 0s - loss: 0.2852 - accuracy: 0.9966 - 212ms/epoch - 21ms/step\n",
            "Epoch 191/200\n",
            "10/10 - 0s - loss: 0.2818 - accuracy: 0.9932 - 213ms/epoch - 21ms/step\n",
            "Epoch 192/200\n",
            "10/10 - 0s - loss: 0.2805 - accuracy: 0.9932 - 218ms/epoch - 22ms/step\n",
            "Epoch 193/200\n",
            "10/10 - 0s - loss: 0.2765 - accuracy: 0.9966 - 218ms/epoch - 22ms/step\n",
            "Epoch 194/200\n",
            "10/10 - 0s - loss: 0.2705 - accuracy: 1.0000 - 215ms/epoch - 22ms/step\n",
            "Epoch 195/200\n",
            "10/10 - 0s - loss: 0.2614 - accuracy: 1.0000 - 228ms/epoch - 23ms/step\n",
            "Epoch 196/200\n",
            "10/10 - 0s - loss: 0.2565 - accuracy: 0.9966 - 214ms/epoch - 21ms/step\n",
            "Epoch 197/200\n",
            "10/10 - 0s - loss: 0.2556 - accuracy: 0.9966 - 219ms/epoch - 22ms/step\n",
            "Epoch 198/200\n",
            "10/10 - 0s - loss: 0.2529 - accuracy: 0.9932 - 207ms/epoch - 21ms/step\n",
            "Epoch 199/200\n",
            "10/10 - 0s - loss: 0.2489 - accuracy: 0.9932 - 211ms/epoch - 21ms/step\n",
            "Epoch 200/200\n",
            "10/10 - 0s - loss: 0.2448 - accuracy: 0.9966 - 218ms/epoch - 22ms/step\n",
            "10/10 [==============================] - 1s 7ms/step - loss: 0.2297 - accuracy: 1.0000\n",
            "model.evaluate : [0.22974929213523865, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자열 생성\n",
        "def sequence_gen_func(model, t, current_word, n):\n",
        "  init_word = current_word\n",
        "  sentence= ''\n",
        "  for _ in range(n):\n",
        "    encoded = t.texts_to_sequences([current_word])[0]\n",
        "    encoded = pad_sequences([encoded], maxlen=max_len-1,padding='pre')\n",
        "    result = np.argmax(model.predict(encoded, verbose=0), axis=-1)\n",
        "    # 예측 단어 찾기\n",
        "    for word, index in t.word_index.items():\n",
        "      # print(word, index)\n",
        "      if index == result: #예측한 단어의 인덱스와 동일한 단어가 있다면\n",
        "        break # 해당 단어가 예측단어이므로 break\n",
        "\n",
        "      current_word = current_word + ' ' + word\n",
        "      sentence = sentence + ' ' + word\n",
        "    sentence = init_word + sentence\n",
        "    return sentence\n",
        "\n",
        "# print(sequence_gen_func(model,tok,'눈이',5))\n",
        "# print(sequence_gen_func(model,tok,'그의',8))\n",
        "# print(sequence_gen_func(model,tok,'그의',10))\n",
        "# print(sequence_gen_func(model,tok,'거리에',3))\n",
        "\n",
        "print(sequence_gen_func(model,tok,'희망퇴직',5))\n",
        "print(sequence_gen_func(model,tok,'은행권이',10))\n",
        "print(sequence_gen_func(model,tok,'은행권이',20))\n",
        "print(sequence_gen_func(model,tok,'은행원들은',5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs2ooz2l0-Uu",
        "outputId": "7f397984-807a-486d-f55d-09ef5bce23fb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "희망퇴직 희망퇴직 이상 희망퇴직을 만 수 눈치만 보는 중이다 지난달 신청을 올해 것은 퇴직금 때문이다 5대 희망 특별 퇴직금을 넘는 조건이 때 것이다 조건을 노조 상황”이라고 직원은 치 일반 은행권이 연말\n",
            "은행권이 희망퇴직 이상 희망퇴직을 만 수 눈치만 보는 중이다 지난달 신청을 올해 것은 퇴직금 때문이다 5대 희망 특별 퇴직금을 넘는 조건이 때 것이다 조건을 노조 상황”이라고 직원은 치 일반 은행권이\n",
            "은행권이 희망퇴직 이상 희망퇴직을 만 수 눈치만 보는 중이다 지난달 신청을 올해 것은 퇴직금 때문이다 5대 희망 특별 퇴직금을 넘는 조건이 때 것이다 조건을 노조 상황”이라고 직원은 치 일반 은행권이\n",
            "은행원들은\n"
          ]
        }
      ]
    }
  ]
}