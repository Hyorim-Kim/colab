{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWiOdEb938J0sJu3INwt5C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hyorim-Kim/numpi/blob/main/tfc32grapheme.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPoGg5NL5T2s",
        "outputId": "9cc9b07f-1e7d-462a-c450-e1bec14390a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jamotools\n",
            "  Downloading jamotools-0.1.10-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from jamotools) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from jamotools) (1.16.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from jamotools) (0.18.3)\n",
            "Installing collected packages: jamotools\n",
            "Successfully installed jamotools-0.1.10\n"
          ]
        }
      ],
      "source": [
        "# token이 자소 단위인 데이터로 자연어 생성 모델 작성\n",
        "!pip install jamotools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jamotools\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "path = keras.utils.get_file(\n",
        "    'rnn_short_toji.txt',\n",
        "    origin='https://raw.githubusercontent.com/pykwon/etc/master/rnn_test_toji.txt')\n",
        "text = open(path, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "print('전체 글자 수 : {}'.format(len(text)))\n",
        "print(text[:100])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WvVjs_l5ntp",
        "outputId": "93bbb051-a7b9-4762-b391-18d99f30cbaa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 글자 수 : 695685\n",
            "제 1 편 어둠의 발소리\r\n",
            "1897년의 한가위.\r\n",
            "까치들이 울타리 안 감나무에 와서 아침 인사를 하기도 전에, 무색 옷에 댕기꼬리를 늘인 \r\n",
            "아이들은 송편을 입에 물고 마을길을 쏘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한글 자료 자모 단위로 분리, 한자에는 영향이 없다.\n",
        "# s_split = jamotools.split_syllables(text[:100])\n",
        "# print(s_split)\n",
        "# rever = jamotools.join_jamos(s_split)\n",
        "# print(rever)\n",
        "# print(text[:50] == rever)\n",
        "\n",
        "train_text_x = jamotools.split_syllables(text)\n",
        "# 단어 사전\n",
        "vocab = sorted(set(train_text_x))\n",
        "vocab.append('UNK')\n",
        "print('{} unique words'.format(len(vocab)))\n",
        "\n",
        "word2idx = {w:i for i, w in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "print(idx2char)\n",
        "text_as_int = np.array([char2idx[c] for c in train_text_x])\n",
        "print(text_as_int)\n",
        "print(len(text_as_int))\n",
        "print(train_text_x[:20])\n",
        "print(text_as_int[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "id": "6Hpg-wvZ6TNN",
        "outputId": "1a3dda01-31d5-4699-cd6f-d4a066b3506c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179 unique words\n",
            "['\\n' '\\r' ' ' '!' '\"' \"'\" '(' ')' ',' '-' '.' '/' '0' '1' '2' '3' '4' '5'\n",
            " '6' '7' '8' '9' ':' '?' 'M' 'X' 'a' 'c' 'd' 'f' 'k' 'l' 'n' 'o' 'p' 'r'\n",
            " 't' 'w' '×' '―' '‘' '’' '“' '”' '…' '\\u3000' 'ㄱ' 'ㄲ' 'ㄳ' 'ㄴ' 'ㄵ' 'ㄶ' 'ㄷ'\n",
            " 'ㄸ' 'ㄹ' 'ㄺ' 'ㄻ' 'ㄼ' 'ㄽ' 'ㄾ' 'ㄿ' 'ㅀ' 'ㅁ' 'ㅂ' 'ㅃ' 'ㅄ' 'ㅅ' 'ㅆ' 'ㅇ' 'ㅈ' 'ㅉ'\n",
            " 'ㅊ' 'ㅋ' 'ㅌ' 'ㅍ' 'ㅎ' 'ㅏ' 'ㅐ' 'ㅑ' 'ㅒ' 'ㅓ' 'ㅔ' 'ㅕ' 'ㅖ' 'ㅗ' 'ㅘ' 'ㅙ' 'ㅚ' 'ㅛ'\n",
            " 'ㅜ' 'ㅝ' 'ㅞ' 'ㅟ' 'ㅠ' 'ㅡ' 'ㅢ' 'ㅣ' '一' '下' '主' '事' '亡' '人' '代' '佛' '刑' '割'\n",
            " '化' '匠' '千' '呪' '啓' '善' '四' '地' '壁' '壽' '天' '太' '妄' '婚' '子' '孫' '寺' '山'\n",
            " '工' '布' '常' '平' '年' '役' '性' '情' '惡' '意' '方' '日' '春' '杖' '榮' '母' '氣' '水'\n",
            " '池' '淨' '無' '燈' '父' '眞' '示' '祈' '祭' '私' '童' '竹' '笠' '籍' '精' '絶' '置' '者'\n",
            " '萬' '術' '衣' '谷' '身' '迷' '造' '銀' '錫' '長' '陷' '電' '順' '食' '金' '落' '來' 'UNK']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b8a32ffb72d3>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0midx2char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx2char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtext_as_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_text_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_as_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_as_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-b8a32ffb72d3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0midx2char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx2char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtext_as_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_text_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_as_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_as_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'char2idx' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset 작성\n",
        "seq_length = 80  # 80개의 자모가 주어질 경우 다음 자모를 예측\n",
        "example_per_epoch = len(text_as_int) // seq_length\n",
        "print(example_per_epoch)\n",
        "\n",
        "import tensorflow as tf\n",
        "sentence_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)  # 전체가 아니라 부분 데이터 읽기\n",
        "sentence_dataset = sentence_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "# seq_length + 1 : 처음 25개 단어(feature)와 그 뒤에 나오는 정답(label)이 될 한 단어를 합쳐 반환하기 위함\n",
        "# drop_remainder=True : 마지막 배치 크기를 무시\n",
        "\n",
        "for item in sentence_dataset.take(1):  # batch를 한번씩 불러옴\n",
        "  print(item.numpy())\n",
        "  print(idx2word[item.numpy()])\n",
        "\n",
        "print()\n",
        "def split_input_target(chunk):\n",
        "  return [chunk[:-1], chunk[:-1]]  # [25단어] [1단어]\n",
        "\n",
        "train_dataset = sentence_dataset.map(split_input_target)\n",
        "\n",
        "for x, y in train_dataset.take(1):\n",
        "  print(idx2word[x.numpy()])\n",
        "  print(x.numpy())\n",
        "  print(idx2word[y.numpy()])\n",
        "  print(y.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DmR36yB9QVU",
        "outputId": "74fc96ff-b8ed-4b88-cd7b-079fc4b0171b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16815\n",
            "[69 81  2 13  2 74 82 49  2 68 80 52 89 62 68 95  2 63 76 54 66 84 54 96\n",
            "  1  0 13 20 21 19 49 82 49 68 95  2 75 76 49 46 76 68 92 10  1  0 47 76\n",
            " 71 96 52 94 54 68 96  2 68 89 54 73 76 54 96  2 68 76 49  2 46 76 62 49\n",
            " 76 62 89 68 81  2 68 85 66]\n",
            "['ㅈ' 'ㅔ' ' ' '1' ' ' 'ㅍ' 'ㅕ' 'ㄴ' ' ' 'ㅇ' 'ㅓ' 'ㄷ' 'ㅜ' 'ㅁ' 'ㅇ' 'ㅢ' ' ' 'ㅂ'\n",
            " 'ㅏ' 'ㄹ' 'ㅅ' 'ㅗ' 'ㄹ' 'ㅣ' '\\r' '\\n' '1' '8' '9' '7' 'ㄴ' 'ㅕ' 'ㄴ' 'ㅇ' 'ㅢ' ' '\n",
            " 'ㅎ' 'ㅏ' 'ㄴ' 'ㄱ' 'ㅏ' 'ㅇ' 'ㅟ' '.' '\\r' '\\n' 'ㄲ' 'ㅏ' 'ㅊ' 'ㅣ' 'ㄷ' 'ㅡ' 'ㄹ' 'ㅇ'\n",
            " 'ㅣ' ' ' 'ㅇ' 'ㅜ' 'ㄹ' 'ㅌ' 'ㅏ' 'ㄹ' 'ㅣ' ' ' 'ㅇ' 'ㅏ' 'ㄴ' ' ' 'ㄱ' 'ㅏ' 'ㅁ' 'ㄴ'\n",
            " 'ㅏ' 'ㅁ' 'ㅜ' 'ㅇ' 'ㅔ' ' ' 'ㅇ' 'ㅘ' 'ㅅ']\n",
            "\n",
            "['ㅈ' 'ㅔ' ' ' '1' ' ' 'ㅍ' 'ㅕ' 'ㄴ' ' ' 'ㅇ' 'ㅓ' 'ㄷ' 'ㅜ' 'ㅁ' 'ㅇ' 'ㅢ' ' ' 'ㅂ'\n",
            " 'ㅏ' 'ㄹ' 'ㅅ' 'ㅗ' 'ㄹ' 'ㅣ' '\\r' '\\n' '1' '8' '9' '7' 'ㄴ' 'ㅕ' 'ㄴ' 'ㅇ' 'ㅢ' ' '\n",
            " 'ㅎ' 'ㅏ' 'ㄴ' 'ㄱ' 'ㅏ' 'ㅇ' 'ㅟ' '.' '\\r' '\\n' 'ㄲ' 'ㅏ' 'ㅊ' 'ㅣ' 'ㄷ' 'ㅡ' 'ㄹ' 'ㅇ'\n",
            " 'ㅣ' ' ' 'ㅇ' 'ㅜ' 'ㄹ' 'ㅌ' 'ㅏ' 'ㄹ' 'ㅣ' ' ' 'ㅇ' 'ㅏ' 'ㄴ' ' ' 'ㄱ' 'ㅏ' 'ㅁ' 'ㄴ'\n",
            " 'ㅏ' 'ㅁ' 'ㅜ' 'ㅇ' 'ㅔ' ' ' 'ㅇ' 'ㅘ']\n",
            "[69 81  2 13  2 74 82 49  2 68 80 52 89 62 68 95  2 63 76 54 66 84 54 96\n",
            "  1  0 13 20 21 19 49 82 49 68 95  2 75 76 49 46 76 68 92 10  1  0 47 76\n",
            " 71 96 52 94 54 68 96  2 68 89 54 73 76 54 96  2 68 76 49  2 46 76 62 49\n",
            " 76 62 89 68 81  2 68 85]\n",
            "['ㅈ' 'ㅔ' ' ' '1' ' ' 'ㅍ' 'ㅕ' 'ㄴ' ' ' 'ㅇ' 'ㅓ' 'ㄷ' 'ㅜ' 'ㅁ' 'ㅇ' 'ㅢ' ' ' 'ㅂ'\n",
            " 'ㅏ' 'ㄹ' 'ㅅ' 'ㅗ' 'ㄹ' 'ㅣ' '\\r' '\\n' '1' '8' '9' '7' 'ㄴ' 'ㅕ' 'ㄴ' 'ㅇ' 'ㅢ' ' '\n",
            " 'ㅎ' 'ㅏ' 'ㄴ' 'ㄱ' 'ㅏ' 'ㅇ' 'ㅟ' '.' '\\r' '\\n' 'ㄲ' 'ㅏ' 'ㅊ' 'ㅣ' 'ㄷ' 'ㅡ' 'ㄹ' 'ㅇ'\n",
            " 'ㅣ' ' ' 'ㅇ' 'ㅜ' 'ㄹ' 'ㅌ' 'ㅏ' 'ㄹ' 'ㅣ' ' ' 'ㅇ' 'ㅏ' 'ㄴ' ' ' 'ㄱ' 'ㅏ' 'ㅁ' 'ㄴ'\n",
            " 'ㅏ' 'ㅁ' 'ㅜ' 'ㅇ' 'ㅔ' ' ' 'ㅇ' 'ㅘ']\n",
            "[69 81  2 13  2 74 82 49  2 68 80 52 89 62 68 95  2 63 76 54 66 84 54 96\n",
            "  1  0 13 20 21 19 49 82 49 68 95  2 75 76 49 46 76 68 92 10  1  0 47 76\n",
            " 71 96 52 94 54 68 96  2 68 89 54 73 76 54 96  2 68 76 49  2 46 76 62 49\n",
            " 76 62 89 68 81  2 68 85]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = example_per_epoch // BATCH_SIZE\n",
        "BUFFER_SIZE = 5000\n",
        "\n",
        "# shuffle을 사용하면 epoch 마다 Dataset을 섞을 수 있다. 과적합 방지에 효과적\n",
        "train_dataset = train_dataset.shuffle(buffer_size=BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "total_chars = len(vocab)\n",
        "print(total_chars)  # 53658\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(total_chars, 100, input_length=seq_length), # 밀집벡터, 100차원으로\n",
        "    tf.keras.layers.LSTM(units=256, return_sequences=True),\n",
        "    tf.keras.layers.Dropout(rate=0.2),\n",
        "    tf.keras.layers.LSTM(units=256),\n",
        "    tf.keras.layers.Dropout(rate=0.2),\n",
        "    tf.keras.layers.Dense(units=total_chars, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1lx1atb-WD4",
        "outputId": "66a114bd-7a07-4e63-d671-2ecb0e78df7f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 80, 100)           17900     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 80, 256)           365568    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 80, 256)           0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 256)               525312    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 179)               46003     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 954783 (3.64 MB)\n",
            "Trainable params: 954783 (3.64 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 단위 생성 모델 학습\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def testmodelFunc(epoch, logs):\n",
        "  if epoch % 5 != 0 and epoch != 49:  # 5의 배수이거나 49이면 처리\n",
        "    return\n",
        "\n",
        "  test_sentence = text[:48]\n",
        "  test_sentence = jamotools.split_syllables(test_sentence)\n",
        "\n",
        "  next_chars = 300\n",
        "  for _ in range(next_chars):\n",
        "    test_text_x = test_sentence.split(' ')[-seq_length:]  # 임의의 문장 뒤에서부터 seq_length(25) 만큼 선택\n",
        "    test_text_x = np.array([word2idx[c] if c in word2idx else word2idx['UNK'] for c in test_text_x])\n",
        "    test_text_x = pad_sequences([test_text_x], maxlen=seq_length, padding='pre', value=word2idx['UNK'])\n",
        "    output_idx = np.argmax(model.predict(test_text_x)[0])  # 출력값 중에서 가장 값이 큰 인덱스 반환\n",
        "    test_sentence += idx2char[output_idx]  # 출력 단어는 test_sentence에 누적해 다음작업의 입력으로 활용\n",
        "  print()\n",
        "  print(test_sentence)\n",
        "  print()\n",
        "\n",
        "# epoch이 끝날 때마다 testmodelFunc를 호출해 진행 결과를 출력\n",
        "# fit 할 때(학습 도중) 학습 데이터가 predict 되는 과정을 확인해가며 작업하고 싶을 때 사용!\n",
        "testModelCb = tf.keras.callbacks.LambdaCallback(on_epoch_end=testmodelFunc)\n",
        "\n",
        "# repeat() : input을 반복, 1개의 epoch의 끝과 다음 epoch의 시작에 상관없이 인자 만큼 반복함\n",
        "history = model.fit(train_dataset.repeat(), epochs=50,\n",
        "                steps_per_epoch = steps_per_epoch,  # 한 에폭에 사용할 step 수를 지정. ex) 총 45개 sample이 있고 배치사이즈가 3이라면 15스텝으로 지정\n",
        "                callbacks=[testModelCb], verbose=2)\n"
      ],
      "metadata": {
        "id": "RsDCrs5x-u26"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}