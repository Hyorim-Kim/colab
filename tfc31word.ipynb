{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPW9HkwsHX4VxNuDH6rkgty",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hyorim-Kim/colab/blob/main/tfc31word.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeSAdN2DBNvk",
        "outputId": "0c0849f4-edb2-42b2-c838-27a4c34c5773"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 글자 수 : 695685\n",
            "제 1 편 어둠의 발소리\r\n",
            "1897년의 한가위.\r\n",
            "까치들이 울타리 안 감나무에 와서 아침 인사를 하기도 전에, 무색 옷에 댕기꼬리를 늘인 \r\n",
            "아이들은 송편을 입에 물고 마을길을 쏘\n",
            "['제 1 편 어둠의 발소리', '1897년의 한가위', '까치들이 울타리 안 감나무에 와서 아침 인사를 하기도 전에, 무색 옷에 댕기꼬리를 늘인 ', '아이들은 송편을 입에 물고 마을길을 쏘다니며 기뻐서 날뛴다 어른들은 해가 중천에서 좀 ', '기울어질 무렵 이래야, 차례를 치러야 했고 성묘를 해야 했고 이웃끼리 음식을 나누다 보면 ']\n",
            "['제', '1', '편', '어둠의', '발소리']\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "path = keras.utils.get_file(\n",
        "    'rnn_short_toji.txt',\n",
        "    origin='https://raw.githubusercontent.com/pykwon/etc/master/rnn_test_toji.txt')\n",
        "text = open(path, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "print('전체 글자 수 : {}'.format(len(text)))\n",
        "print(text[:100])\n",
        "\n",
        "# 데이터 정제\n",
        "import re\n",
        "\n",
        "def clean_str_func(string):\n",
        "  string = re.sub(r\"[^가-힣0-9(), ?!]\",\"\",string)\n",
        "  string = re.sub(r\"!\",\"! \",string)\n",
        "  string = re.sub(r\"\\(\",\"\",string)\n",
        "  string = re.sub(r\"\\)\",\"\",string)\n",
        "  string = re.sub(r\"\\?\",\"? \",string)\n",
        "  string = re.sub(r\"\\s{2,}\",\" \",string) # 공백 두개 이상이면 공백 하나로 바꾸기\n",
        "  string = re.sub(r\"\\'\",\"\",string)\n",
        "  return string\n",
        "\n",
        "# print(clean_str_func(\"abc,,?! AB 12 나는 '  (간다) 34\"))\n",
        "\n",
        "train_text = text.split('\\n')\n",
        "train_text = [clean_str_func(sentence) for sentence in train_text]\n",
        "print(train_text[:5])\n",
        "\n",
        "train_text_x = []\n",
        "for sen in train_text:\n",
        "  train_text_x.extend(sen.split(' '))\n",
        "  train_text_x.append('\\n')\n",
        "\n",
        "train_text_x = [word for word in train_text_x if word != ''] # 단어1 단어2 단어3 -> 공백을 기준으로 단어를 나눔\n",
        "print(train_text_x[:5])\n",
        "# train_text_y = []\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 사전\n",
        "vocab = sorted(set(train_text_x))\n",
        "vocab.append('UNK') # 단어 사전에 존재하지 않는 토큰은 'UNK'로 처리\n",
        "# 특수 토근 <unk> 토크나이저가 모르는 단어를 만나면 unknown으로 처리하기 위한 처리용 토큰입니다.\n",
        "# <eos>, <pad> ,<sep>, <mask>\n",
        "print('{}unique words'.format(len(vocab))) # token(word) : 54048개\n",
        "print(train_text_x[:20])\n",
        "\n",
        "# 단어 인덱싱\n",
        "word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "# print(word2idx)\n",
        "idx2word = np.array(vocab)\n",
        "print(idx2word) # ['\\n' ',' ',,?' ... '힘찼으며' '힝' 'UNK']\n",
        "text_as_int = np.array([word2idx[c] for c in train_text_x])\n",
        "print(text_as_int) # [43654     6 50166 ... 40598     0     0]\n",
        "\n",
        "print(train_text_x[:20])\n",
        "print(text_as_int[:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdxGIe3wHvnS",
        "outputId": "ce5a2368-0d78-4882-ec6f-b0858d7b07fd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54048unique words\n",
            "['제', '1', '편', '어둠의', '발소리', '\\n', '1897년의', '한가위', '\\n', '까치들이', '울타리', '안', '감나무에', '와서', '아침', '인사를', '하기도', '전에,', '무색', '옷에']\n",
            "['\\n' ',' ',,?' ... '힘찼으며' '힝' 'UNK']\n",
            "[43654     6 50166 ... 40598     0     0]\n",
            "['제', '1', '편', '어둠의', '발소리', '\\n', '1897년의', '한가위', '\\n', '까치들이', '울타리', '안', '감나무에', '와서', '아침', '인사를', '하기도', '전에,', '무색', '옷에']\n",
            "[43654     6 50166 34431 21971     0    19 51388     0  6925 38396 32980\n",
            "  1062 37351 32849 40304 50929 43182 20097 37292]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset 작성\n",
        "seq_length = 25 # 25개의 단어가 주어질 경우 다음 단어를 예측\n",
        "example_per_epoch = len(text_as_int) // seq_length\n",
        "print(example_per_epoch)\n",
        "\n",
        "import tensorflow as tf\n",
        "sentence_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) # 전체가 아니라 부분 데이터 읽기\n",
        "sentence_dataset = sentence_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "# seq_length + 1 : 처음 25개 단어와 그 뒤에 나오는 정답(label)이 될 한 단어를 합쳐 반환하기 위함\n",
        "# drop_remainder=True : 마지막 배치 크기를 무시\n",
        "\n",
        "for item in sentence_dataset.take(1): # batch를 한번씩 불러옴 -> 데이터가 양이 많을 때 이렇게 불러옴(청크처럼)\n",
        "  print(item.numpy())\n",
        "  print(idx2word[item.numpy()])\n",
        "\n",
        "def split_input_target(chunk):\n",
        "  return [chunk[:-1],chunk[-1]] # [25단어].1단어\n",
        "\n",
        "train_dataset = sentence_dataset.map(split_input_target)\n",
        "\n",
        "for x, y in train_dataset.take(1):\n",
        "  print(idx2word[x.numpy()])\n",
        "  print(x.numpy())\n",
        "  print(idx2word[y.numpy()])\n",
        "  print(y.numpy())\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxL5PMMfPnGY",
        "outputId": "7740ff0b-0bb2-4fee-cdbf-93930b928709"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7093\n",
            "[43654     6 50166 34431 21971     0    19 51388     0  6925 38396 32980\n",
            "  1062 37351 32849 40304 50929 43182 20097 37292 12523 11092     0 32768\n",
            " 29657 40995]\n",
            "['제' '1' '편' '어둠의' '발소리' '\\n' '1897년의' '한가위' '\\n' '까치들이' '울타리' '안' '감나무에'\n",
            " '와서' '아침' '인사를' '하기도' '전에,' '무색' '옷에' '댕기꼬리를' '늘인' '\\n' '아이들은' '송편을' '입에']\n",
            "['제' '1' '편' '어둠의' '발소리' '\\n' '1897년의' '한가위' '\\n' '까치들이' '울타리' '안' '감나무에'\n",
            " '와서' '아침' '인사를' '하기도' '전에,' '무색' '옷에' '댕기꼬리를' '늘인' '\\n' '아이들은' '송편을']\n",
            "[43654     6 50166 34431 21971     0    19 51388     0  6925 38396 32980\n",
            "  1062 37351 32849 40304 50929 43182 20097 37292 12523 11092     0 32768\n",
            " 29657]\n",
            "입에\n",
            "40995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = example_per_epoch // BATCH_SIZE\n",
        "BUFFER_SIZE = 5000\n",
        "\n",
        "# suffle을 사용하면 epoch마다 Dataset을 섞을 수 있다.\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "total_words = len(vocab)\n",
        "print(total_words) # 54048\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(total_words, 100, input_length=seq_length),\n",
        "    tf.keras.layers.LSTM(units=256, return_sequences=True),\n",
        "    tf.keras.layers.Dropout(rate=0.2),\n",
        "    tf.keras.layers.LSTM(units=256),\n",
        "    tf.keras.layers.Dense(units=total_words, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=total_words, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "R1oQ-fUAU1Qk",
        "outputId": "fd8bcd18-d59c-4209-c25f-c41033122222"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54048\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-553a25fbc7f5>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 54048\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m model = tf.keras.Sequential([\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[1;32m   2100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnonce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2101\u001b[0m                 \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateless_fold_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2102\u001b[0;31m             return tf.random.stateless_uniform(\n\u001b[0m\u001b[1;32m   2103\u001b[0m                 \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2104\u001b[0m                 \u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 단위 생성 모델 학습\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def testmodelFunc(epoch, logs):\n",
        "  if epoch % 5 != 0 and epoch != 49: # 5의 배수이거나 49이면 처리\n",
        "    return\n",
        "\n",
        "  test_sentence = train_text[0]\n",
        "\n",
        "  next_words = 100\n",
        "  for _ in range(next_words):\n",
        "    test_text_x = test_sentence.split(' ')[seq_length:] # 임의의 문장 뒤에서 부터 seq_length(25) 만큼 선택\n",
        "    test_text_x = np.array(word2idx[c] if c in word2idx else word2idx['UNK'] for c in test_text_x)\n",
        "    test_text_x = pad_sequences([test_text_x], maxlen=seq_length, padding='pre', value=word2idx['UNK'])\n",
        "    ouput_idx = np.argmax(model.predict(test_text_x)[0]) # 출력 값 중에서 가장 값이 큰 인덱스 반환\n",
        "    test_sentence += ' ' + idx2word[ouput_idx[0]] # 출력 단어는 test_sentence에 누적해 다음 작업의 입력으로 활용\n",
        "  print()\n",
        "  print(test_sentence)\n",
        "  print()\n",
        "\n",
        "# epoch이 끝날 때 마다 testmodelFunc 호출해 진행 결과를 출력\n",
        "# fit 할 때(학습 도중) 학습 데이터가 predict 되는 과정을 확인해 가며 작업하고 싶을 때 사용\n",
        "testModelCb = tf.keras.callbacks.LambdaCallback(on_epoch_end=testmodelFunc)\n",
        "\n",
        "# repeat() : input을 반복. 1개의 epoch의 끝과 다음 epoch의 시작에 상관없이 인자 만큼 반복함\n",
        "history = model.fit(train_dataset.repeat(), epochs=50,\n",
        "                    steps_per_epoch=steps_per_epoch, # 한 에폭에 사용할 step수를 지정. ex) 총 45개 sample이 있고 배치 사이즈가 3이라면 15step으로 지정\n",
        "                    )"
      ],
      "metadata": {
        "id": "qE44J0ECrC_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(history.history['loss'][-1])\n",
        "print(history.history['accuracy'][-1])\n",
        "\n",
        "model.save('tfc31model.hdf5')"
      ],
      "metadata": {
        "id": "--1pX5FPxkHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('tfc31model.hdf5')\n",
        "\n",
        "# 임의의 문장을 사용해 생성된 새로운 문장 확인\n",
        "test_sentence = '이날은 수수개비를 꺾어도 아이들은 매를 맞지 않는다'\n",
        "\n",
        "next_words = 100\n",
        "\n",
        "for _ in range(next_words):\n",
        "    test_text_x = test_sentence.split(' ')[-seq_length:]  # 임의의 문장 뒤에서부터 seq_length(25) 만큼 선택\n",
        "    test_text_x = np.array([word2idx[c] if c in word2idx else word2idx['UNK'] for c in test_text_x])\n",
        "    test_text_x = pad_sequences([test_text_x], maxlen=seq_length, padding='pre', value=word2idx['UNK'])\n",
        "    output_idx = np.argmax(model.predict(test_text_x)[0])  # 출력값 중에서 가장 값이 큰 인덱스 반환\n",
        "    test_sentence += ' ' + idx2word[output_idx]  # 출력 단어는 test_sentence에 누적해 다음작업의 입력으로 활용\n",
        "\n",
        "print(test_sentence)"
      ],
      "metadata": {
        "id": "DG_LqRAM3dmv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}